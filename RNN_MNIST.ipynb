{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network for MNIST\n",
    "\n",
    "This notebook implements a (dynamic) multilayer recurrent neural network (RNN) using long short term memory (LSTM) units and TensorFlow. It contains detailed explanations for each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mnist import MNIST\n",
    "\n",
    "% matplotlib inline\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset\n",
    "\n",
    "The MNIST dataset contains handwritten digits and is available [here](http://yann.lecun.com/exdb/mnist/). To proceed you first have to download the training data and labels and the test data and labels. Next, unpack them into a folder named \"mnist\" in your home directory. To make the data extraction work, you will have to rename the dots to -, for example t10k-images.idx3-ubyte must be renamed to t10k-images-idx3-ubyte.\n",
    "\n",
    "To read the MNIST files we will use the [python mnist package](https://pypi.org/project/python-mnist/). You can install the package via\n",
    "\n",
    "```bash\n",
    "pip install python-mnist\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = os.path.expanduser(\"~/mnist/\")\n",
    "mnist = MNIST(path_to_data)\n",
    "\n",
    "x_train, y_train = mnist.load_training()\n",
    "x_test, y_test = mnist.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (60000, 784)\n",
      "Training labels shape: (60000,)\n",
      "Test set shape: (10000, 784)\n",
      "Test set shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"Training set shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "\n",
    "print(f\"Test set shape: {x_test.shape}\")\n",
    "print(f\"Test set shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying model and dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "eta = 0.01 # learning rate\n",
    "n_epochs = 4\n",
    "n_input = 28\n",
    "n_classes = 10\n",
    "batch_size = 100\n",
    "n_train_samples = x_train.shape[0]\n",
    "n_test_samples = x_test.shape[0]\n",
    "\n",
    "n_train_batches = n_train_samples // batch_size\n",
    "n_test_batches = n_test_samples // batch_size\n",
    "\n",
    "# Network parameters\n",
    "n_hidden = 50 # number of hidden units per layer\n",
    "n_layers = 3 # number of layers \n",
    "n_steps = 28 # number of truncated backprop steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating one-hot encoded labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_one_hot = np.zeros((n_train_samples, n_classes))\n",
    "y_train_one_hot[np.arange(n_train_samples), y_train] = 1\n",
    "\n",
    "y_test_one_hot = np.zeros((n_test_samples, n_classes))\n",
    "y_test_one_hot[np.arange(n_test_samples), y_test] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network\n",
    "\n",
    "### Network architecture\n",
    "\n",
    "The basic architecture of a recurrent network looks as follows ([source](http://www.deeplearningbook.org/)). \n",
    "\n",
    "![title](figures/basic_rnn.png)\n",
    "\n",
    "\n",
    "As visibile in the figure, the state $h^{(t)}$  of the network depends both on the input $x^{(t)}$ and the previous state $h^{(t-1)}$.\n",
    "It is computed as follows:\n",
    "\n",
    "$$ h^{(t)} = \\sigma(U x^{(t)} + W h^{(t-1)} + b) $$\n",
    "\n",
    "with $\\sigma$ beign the $\\tanh$ in our implementation.\n",
    "\n",
    "\n",
    "The output is computed as: \n",
    "\n",
    "$$ o^{(t)} = V h^{(t)} + c $$\n",
    "$$ \\hat{y}^{(t)} = \\text{softmax}(o^{(t)}) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated backpropagation\n",
    "\n",
    "A recurrent neural network is designed in a way such that the output at a certain time step depends on arbitrarily distant inputs. In other words: when building an RNN in TensorFlow, the graph would have to be as wide as the input sequence.\n",
    "Unfortunately, this makes backpropagation computation both expensive and ineffective because gradients propagated over many time steps tend to either vanish (most of the time) or explode.\n",
    "\n",
    "A common solution to this problem is to create an \"unrolled\" version of the recurrent network that contains a fixed number (*n_steps*) of RNN inputs and outputs. In other words: backpropagation is \"truncated\" such that errors are only backpropagated for a fixed number of steps. A higher number of steps enables capturing long-term dependencies but is also more expensive (both regarding memory and computation).\n",
    "\n",
    "The model is then trained on this finite approximation of the recurrent network. Accordingly, at each time step the network is fed with inputs of length *n_steps*. The backward pass is performed after each input block. A short explanation is given on TensorFlow's [website](https://www.tensorflow.org/tutorials/recurrent#truncated-backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic vs. static RNN \n",
    "\n",
    "Tensorflow has two different versions of the RNN functions, namely *tf.nn.static_rnn* and *tf.nn.dynamic_rnn*. \n",
    "\n",
    "*tf.nn.static_rnn* creates an unrolled graph for a *fixed* RNN length. For example, when calling *tf.nn.rnn* with an input sequence of length 200, a static graph with 200 time steps is created. This has the disadvantage that we cannot feed longer or shorter sequences into the network than originally specified.\n",
    "\n",
    "*tf.nn.dynamic_rnn* solves this problem. It dynamically constructs the graph when it's executed, reducing the time requirement for graph creation and allowing for the input batches to vary in size.\n",
    "\n",
    "One difference between the two functions is the form of the input data. Whereas *tf.nn.static_rnn* takes a list of tensors as an input (namely a list of  n_steps tensors with shape (batch_size, input_size), *tf.nn.dynamic_rnn* takes as input the whole tensor of shape (batch_size, n_steps, input_size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cell():\n",
    "    cell = tf.contrib.rnn.LSTMCell(num_units=n_hidden)\n",
    "    return cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create placeholder variables for the input and targets\n",
    "X_placeholder = tf.placeholder(tf.float32, shape=[batch_size, n_steps, n_input])\n",
    "y_placeholder = tf.placeholder(tf.int32, shape=[batch_size, n_classes])\n",
    "\n",
    "# Create placeholder variables for final weight and bias matrix \n",
    "V = tf.Variable(tf.random_normal(shape=[n_hidden, n_classes]))\n",
    "c = tf.Variable(tf.random_normal(shape=[n_classes]))\n",
    "\n",
    "# To create multiple layers we call the MultiRNNCell function that takes \n",
    "# a list of RNN cells as an input and wraps them into a single cell\n",
    "cell = tf.contrib.rnn.MultiRNNCell([make_cell() for _ in range(n_layers)], state_is_tuple=True)\n",
    "\n",
    "# Create a zero-filled state tensor as an initial state\n",
    "init_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# Create a recurrent neural network specified by \"cell\", i.e. unroll the\n",
    "# network.\n",
    "# Returns a list of all previous RNN hidden states and the final states.\n",
    "# final_state contains n_layer LSTMStateTuple that contain both the \n",
    "# final hidden and the cell state of the respective layer.\n",
    "outputs, final_state = tf.nn.dynamic_rnn(cell, \n",
    "                                         X_placeholder, \n",
    "                                         initial_state=init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather final activations\n",
    "\n",
    "Because we are performing sequence *classification*, we are only interested in the output activations of the last timestep. Since tensorflow does not support negative indexing, we first transpose the tensor such that the \"n_steps\" axis is first. Then, we use tf.gather to select the correct slice. This process is illustrated below for the following parameter setting: batch_size=100, n_steps=28, n_hidden=20\n",
    "\n",
    "![title](figures/tensor_transformation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf.transpose(outputs, [1,0,2])\n",
    "last_output = tf.gather(temp, int(temp.get_shape()[0]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network output and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After gathering the final activations we can easily compute the logits\n",
    "# using a single matrix multiplication\n",
    "logits = tf.matmul(last_output, V) + c\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_placeholder,\n",
    "                                                           logits=logits))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(eta).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "We compute the accuracy of our model as follows.\n",
    "First, we use *tf.argmax* which gives us the highest entry in a tensor along some axis. For example, *tf.argmax(logits,1)* gives us the label our model considers to be most likely for each input. The true labels are computed using *tf.argmax(y_placeholder,1)*. In a next step, we compare these two tensors using *tf.equal* resulting in a tensor of boolean values.\n",
    "\n",
    "To compute the accuracy, we first cast the boolean values to floats using *tf.cast*. In a last step, we take the mean of all values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(logits,1), tf.argmax(y_placeholder,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "\n",
    "We train the network for the specified number of epochs. In each epoch, we run through all training examples, computing the loss and accuracy after 100 batches.\n",
    "\n",
    "After the training phase is done, we evaluate our model on the test set and report the average loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Minibatch loss: 2.303   Accuracy: 0.160\n",
      "Minibatch loss: 0.322   Accuracy: 0.910\n",
      "Minibatch loss: 0.295   Accuracy: 0.920\n",
      "Minibatch loss: 0.203   Accuracy: 0.950\n",
      "Minibatch loss: 0.186   Accuracy: 0.970\n",
      "Minibatch loss: 0.091   Accuracy: 0.960\n",
      "\n",
      "Epoch: 1\n",
      "Minibatch loss: 0.138   Accuracy: 0.960\n",
      "Minibatch loss: 0.162   Accuracy: 0.970\n",
      "Minibatch loss: 0.167   Accuracy: 0.940\n",
      "Minibatch loss: 0.064   Accuracy: 0.970\n",
      "Minibatch loss: 0.065   Accuracy: 0.990\n",
      "Minibatch loss: 0.054   Accuracy: 0.980\n",
      "\n",
      "Epoch: 2\n",
      "Minibatch loss: 0.063   Accuracy: 0.980\n",
      "Minibatch loss: 0.141   Accuracy: 0.970\n",
      "Minibatch loss: 0.062   Accuracy: 1.000\n",
      "Minibatch loss: 0.049   Accuracy: 0.990\n",
      "Minibatch loss: 0.059   Accuracy: 0.990\n",
      "Minibatch loss: 0.082   Accuracy: 0.970\n",
      "\n",
      "Epoch: 3\n",
      "Minibatch loss: 0.063   Accuracy: 0.980\n",
      "Minibatch loss: 0.085   Accuracy: 0.960\n",
      "Minibatch loss: 0.084   Accuracy: 0.980\n",
      "Minibatch loss: 0.080   Accuracy: 0.980\n",
      "Minibatch loss: 0.018   Accuracy: 1.000\n",
      "Minibatch loss: 0.022   Accuracy: 1.000\n",
      "\n",
      "Optimization done! Let's calculate the test error\n",
      "\n",
      "Average loss on test set: 0.024\n",
      "Accuracy on test set: 0.990\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # We first have to initialize all variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # TRAINING\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        print()\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        \n",
    "        for i in range(n_train_batches):           \n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "\n",
    "            x_batch = x_train[start:end]        \n",
    "            x_batch = x_batch.reshape((batch_size, n_steps, n_input))\n",
    "            y_batch = y_train_one_hot[start:end]\n",
    "            \n",
    "            _train_step = sess.run(train_step, \n",
    "                                        feed_dict=\n",
    "                                        {X_placeholder:x_batch,\n",
    "                                         y_placeholder:y_batch\n",
    "                                        })\n",
    "            \n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                _loss, _accuracy = sess.run([loss, accuracy],\n",
    "                                 feed_dict={\n",
    "                                     X_placeholder:x_batch,\n",
    "                                     y_placeholder:y_batch\n",
    "                                 })\n",
    "                \n",
    "                print(f\"Minibatch loss: {_loss:.3f}   \" \n",
    "                      f\"Accuracy: {_accuracy:.3f}\")\n",
    "          \n",
    "    print()\n",
    "    print(f\"Optimization done! Let's calculate the test error\")\n",
    "   \n",
    "    # TESTING\n",
    "    losses = []\n",
    "    acc = []\n",
    "\n",
    "    for i in range(n_test_batches):\n",
    "\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "\n",
    "        x_test_batch = x_test[start:end]        \n",
    "        x_test_batch = x_test_batch.reshape((batch_size, n_steps, n_input))\n",
    "        y_test_batch = y_test_one_hot[start:end]\n",
    "\n",
    "\n",
    "        test_loss, test_accuracy, _train_step = sess.run([loss, accuracy, train_step],\n",
    "                                                        feed_dict={\n",
    "                                                            X_placeholder:x_test_batch,\n",
    "                                                            y_placeholder:y_test_batch\n",
    "                                                        })\n",
    "\n",
    "        losses.append(test_loss)\n",
    "        acc.append(test_accuracy)\n",
    "\n",
    "    print()\n",
    "    print(f\"Average loss on test set: {np.mean(test_loss):.3f}\")\n",
    "    print(f\"Accuracy on test set: {np.mean(test_accuracy):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
